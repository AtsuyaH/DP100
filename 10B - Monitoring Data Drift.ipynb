{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データドリフトの監視\n",
    "\n",
    "時間が経つにつれて、モデルは、機能データの変化する傾向により、正確に予測する効果が低下する可能性があります。この現象は「データドリフト」と呼ばれ、必要に応じてモデルを再トレーニングできるように、機械学習ソリューションを監視して検出することが重要です。\n",
    "\n",
    "このラボでは、データセットのデータドリフトモニタリングを設定します。\n",
    "\n",
    "## ワークスペースに接続する\n",
    "\n",
    "最初に行う必要があるのは、Azure ML SDKを使用してワークスペースに接続することです。\n",
    "\n",
    "> **Note**: 前の演習を完了してから、Azureサブスクリプションとの認証済みセッションの有効期限が切れた場合、再認証するように求められます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベースラインデータセットを作成する\n",
    "\n",
    "データドリフトのデータセットを監視するには、*ベースライン*データセット（通常、モデルのトレーニングに使用されるデータセット）を登録して、将来収集されるデータとの比較ポイントとして使用する必要があります。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "\n",
    "# Upload the baseline data\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'],\n",
    "                       target_path='diabetes-baseline',\n",
    "                       overwrite=True, \n",
    "                       show_progress=True)\n",
    "\n",
    "# Create and register the baseline dataset\n",
    "print('Registering baseline dataset...')\n",
    "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
    "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
    "                           name='diabetes baseline',\n",
    "                           description='diabetes baseline data',\n",
    "                           tags = {'format':'CSV'},\n",
    "                           create_new_version=True)\n",
    "\n",
    "print('Baseline dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ターゲットデータセットを作成する\n",
    "\n",
    "時間が経つにつれて、ベースライントレーニングデータと同じ機能を備えた新しいデータを収集できます。この新しいデータをベースラインデータと比較するには、データドリフトを分析する機能を含むターゲットデータセットと、新しいデータが最新であった時点を示すタイムスタンプフィールドを定義する必要があります。これにより、時間間隔でのデータドリフトを測定します。タイムスタンプは、データセット自体のフィールドにすることも、データの保存に使用するフォルダーとファイル名のパターンから派生させることもできます。たとえば、月のフォルダーを含む年のフォルダーと、その日のフォルダーを含むフォルダー階層で新しいデータを保存できます。または、次のようにファイル名に年、月、日をエンコードすることもできます。*data_2020-01-29.csv*;これは、次のコードで採用されているアプローチです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "print('Generating simulated data...')\n",
    "\n",
    "# Load the smaller of the two data files\n",
    "data = pd.read_csv('data/diabetes2.csv')\n",
    "\n",
    "# We'll generate data for the past 6 weeks\n",
    "weeknos = reversed(range(6))\n",
    "\n",
    "file_paths = []\n",
    "for weekno in weeknos:\n",
    "    \n",
    "    # Get the date X weeks ago\n",
    "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
    "    \n",
    "    # Modify data to ceate some drift\n",
    "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
    "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
    "    data['BMI'] = data['BMI'] * 1.1\n",
    "    \n",
    "    # Save the file with the date encoded in the filename\n",
    "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
    "    data.to_csv(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Upload the files\n",
    "path_on_datastore = 'diabetes-target'\n",
    "default_ds.upload_files(files=file_paths,\n",
    "                       target_path=path_on_datastore,\n",
    "                       overwrite=True,\n",
    "                       show_progress=True)\n",
    "\n",
    "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
    "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
    "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
    "                                                       partition_format=partition_format)\n",
    "\n",
    "# Register the target dataset\n",
    "print('Registering target dataset...')\n",
    "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
    "                                                                          name='diabetes target',\n",
    "                                                                          description='diabetes target data',\n",
    "                                                                          tags = {'format':'CSV'},\n",
    "                                                                          create_new_version=True)\n",
    "\n",
    "print('Target dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データドリフトモニターを作成する\n",
    "\n",
    "これで、糖尿病データのデータドリフトモニターを作成する準備が整いました。データドリフトモニターは定期的にまたはオンデマンドで実行され、ベースラインデータセットとターゲットデータセットを比較します。ターゲットデータセットには、新しいデータが徐々に追加されます。\n",
    "\n",
    "データドリフトを監視する機能、監視プロセスの実行に使用する計算ターゲットの名前、データを比較する頻度、アラートがトリガーされるデータドリフトのしきい値、データの収集を可能にする待ち時間（時間単位）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "# set up feature list\n",
    "features = ['Pregnancies', 'Age', 'BMI']\n",
    "\n",
    "# set up data drift detector\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'diabetes-drift-detector', baseline_data_set, target_data_set,\n",
    "                                                      compute_target='aml-cluster', \n",
    "                                                      frequency='Week', \n",
    "                                                      feature_list=features, \n",
    "                                                      drift_threshold=.3, \n",
    "                                                      latency=24)\n",
    "monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モニターのバックフィル\n",
    "\n",
    "6週間のシミュレートされた毎週のデータ収集を含むベースラインデータセットとターゲットデータセットがあります。これを使用してモニターを埋め戻し、元のベースラインとターゲットデータ間のデータドリフトを分析できます。\n",
    "\n",
    "> **Note** バックフィル分析を実行するには計算ターゲットを起動する必要があるため、これには実行に時間がかかる場合があります。ウィジェットが常に更新されてステータスが表示されるとは限らないため、リンクをクリックして、Azure Machine Learning Studioで実験ステータスを確認してください！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データドリフトの分析\n",
    "\n",
    "次のコードを使用して、バックフィル実行で収集された時点のデータドリフトを調べることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_metrics = backfill.get_metrics()\n",
    "for metric in drift_metrics:\n",
    "    print(metric, drift_metrics[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次の手順に従って、[Azure Machine Learning studio](https://ml.azure.com)でデータドリフトメトリックを視覚化することもできます。\n",
    "\n",
    "1. [**データセット**]ページで、[**データセットモニター**]タブを表示します。\n",
    "2. 表示するデータドリフトモニターをクリックします。\n",
    "3. データドリフトメトリックを表示する日付範囲を選択します（列グラフに複数週間のデータが表示されない場合は、1分ほど待ってから[**更新**]をクリックします）。\n",
    "4. 上部の**ドリフトの概要**セクションのチャートを調べてください。これは、全体的なドリフトの大きさとフィーチャごとのドリフトの寄与を示しています。\n",
    "5. 下部の[**機能の詳細**]セクションのグラフをご覧ください。個々の機能のさまざまなドリフトの測定値を確認できます。\n",
    "\n",
    "> **Note**: データドリフトメトリックの理解については、次の[データセットを監視する方法](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets#understanding-data-drift-results)を参照してください。 Azure Machine Learningのドキュメント。\n",
    "\n",
    "## さらに探索する\n",
    "\n",
    "このラボは、データドリフトモニタリングの概念と原理を紹介するように設計されています。データセットを使用したデータドリフトの監視の詳細については、Azure machine Learningドキュメントの[データセットのデータドリフトの検出](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets)を参照してください。 。\n",
    "\n",
    "Azure Kubernetes Service（AKS）クラスターにデプロイされたサービスのデータドリフトモニタリングを構成することもできます。この詳細については、「Azure Kubernetes Service（AKS）にデプロイされたモデルのデータドリフトを検出する」(https://docs.microsoft.com/azure/machine-learning/how-to-monitor-data-drift)を参照してくださいAzure Machine Learningのドキュメント。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
